{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4448e0a",
   "metadata": {},
   "source": [
    "\n",
    "# 머신러닝 기본기 실습 노트북 (Python)\n",
    "\n",
    "이 노트북은 다음 내용을 **실습 중심**으로 다룹니다.\n",
    "\n",
    "1) **[이론] 머신러닝 기본기**\n",
    "- 전치 행렬, 단위 행렬, 역행렬, 미분(수치 미분)\n",
    "\n",
    "2) **[이론] 기본 지도 학습 알고리즘들**\n",
    "- 선형 회귀, 다중 선형 회귀, 다항 회귀, 로지스틱 회귀\n",
    "\n",
    "3) **[이론] 머신러닝 더 빠르고 정확하게**\n",
    "- Normalization / Standardization / Feature Scaling\n",
    "- One-hot encoding\n",
    "- Bias–Variance trade-off\n",
    "- Regularization (L1, L2)\n",
    "- Grid Search\n",
    "\n",
    "> 실행 전: 아래 셀에서 필요 패키지를 확인하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65befe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 환경 점검: 필수 패키지 버전 출력\n",
    "import sys, numpy, pandas, matplotlib\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", numpy.__version__)\n",
    "print(\"Pandas:\", pandas.__version__)\n",
    "print(\"Matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "# 선택: scikit-learn\n",
    "try:\n",
    "    import sklearn\n",
    "    print(\"scikit-learn:\", sklearn.__version__)\n",
    "except Exception as e:\n",
    "    print(\"scikit-learn 미설치 (일부 실습은 설치 필요). pip install scikit-learn 로 설치하세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661e0bb",
   "metadata": {},
   "source": [
    "\n",
    "## 1. 선형대수 기초 실습\n",
    "- 전치(Transpose), 단위(Identity), 역행렬(Inverse)\n",
    "- 수치 미분(numerical gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a79a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# 전치, 단위, 역행렬\n",
    "A = np.array([[3., 2.],\n",
    "              [1., 4.]])\n",
    "print(\"A =\\n\", A)\n",
    "print(\"A^T =\\n\", A.T)\n",
    "\n",
    "I = np.eye(2)\n",
    "print(\"I =\\n\", I)\n",
    "\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(\"A^{-1} =\\n\", A_inv)\n",
    "\n",
    "# 검증: A * A^{-1} ≈ I\n",
    "print(\"A @ A^{-1} =\\n\", A @ A_inv)\n",
    "\n",
    "# 수치 미분: f(x) = x^2 + 3x + 1 의 도함수 f'(x) ≈ (f(x+h)-f(x-h))/(2h)\n",
    "def f(x):\n",
    "    return x**2 + 3*x + 1\n",
    "\n",
    "def numerical_derivative(func, x, h=1e-6):\n",
    "    return (func(x+h) - func(x-h)) / (2*h)\n",
    "\n",
    "for x0 in [-2, 0, 2]:\n",
    "    print(f\"x={x0: .1f}, f'(x) 수치근사={numerical_derivative(f, x0):.6f}, 해석해 f'(x)=2x+3={2*x0+3:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede182b",
   "metadata": {},
   "source": [
    "\n",
    "## 2. 지도학습: 회귀(Regression)\n",
    "\n",
    "- **선형 회귀**: 1차 모델  \n",
    "- **다중 선형 회귀**: 다수의 설명변수  \n",
    "- **다항 회귀**: 다항 특성 변환 후 선형 모델 학습\n",
    "\n",
    "데이터는 `sklearn.datasets.make_regression` 또는 합성 데이터로 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb88a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 2-1) 단변량 선형 회귀\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=15.0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "lin = LinearRegression().fit(X_train, y_train)\n",
    "pred = lin.predict(X_test)\n",
    "print(\"단순 선형 회귀 MSE:\", mean_squared_error(y_test, pred))\n",
    "\n",
    "# 시각화 (단일 플롯, 색상 미지정)\n",
    "plt.figure()\n",
    "plt.scatter(X_test, y_test, label=\"test\")\n",
    "# 정렬 후 예측선\n",
    "order = np.argsort(X_test[:,0])\n",
    "plt.plot(X_test[order], pred[order], label=\"pred\")\n",
    "plt.title(\"단순 선형 회귀\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2-2) 다중 선형 회귀\n",
    "rng = np.random.RandomState(0)\n",
    "n = 300\n",
    "X_multi = np.c_[rng.uniform(-3,3,size=n), rng.uniform(0,5,size=n)]\n",
    "# y = 2*x1 - 1.5*x2 + noise\n",
    "y_multi = 2*X_multi[:,0] - 1.5*X_multi[:,1] + rng.normal(0, 1.0, size=n)\n",
    "\n",
    "Xm_tr, Xm_te, ym_tr, ym_te = train_test_split(X_multi, y_multi, test_size=0.3, random_state=0)\n",
    "lin_multi = LinearRegression().fit(Xm_tr, ym_tr)\n",
    "print(\"다중 선형 회귀 계수:\", lin_multi.coef_, \"절편:\", lin_multi.intercept_)\n",
    "print(\"다중 선형 회귀 MSE:\", mean_squared_error(ym_te, lin_multi.predict(Xm_te)))\n",
    "\n",
    "# 2-3) 다항 회귀 (degree=3 예시)\n",
    "poly_model = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    (\"lin\", LinearRegression())\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "pred_poly = poly_model.predict(X_test)\n",
    "print(\"다항 회귀(deg=3) MSE:\", mean_squared_error(y_test, pred_poly))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test, y_test, label=\"test\")\n",
    "plt.plot(X_test[order], pred_poly[order], label=\"poly deg=3\")\n",
    "plt.title(\"다항 회귀 (deg=3)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404650c4",
   "metadata": {},
   "source": [
    "\n",
    "## 3. 지도학습: 분류(Classification) – 로지스틱 회귀\n",
    "\n",
    "- 이진 분류 데이터 생성 후 학습/평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a622bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "X_cls, y_cls = make_classification(n_samples=500, n_features=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "Xc_tr, Xc_te, yc_tr, yc_te = train_test_split(X_cls, y_cls, test_size=0.3, random_state=42)\n",
    "\n",
    "logr = LogisticRegression().fit(Xc_tr, yc_tr)\n",
    "proba = logr.predict_proba(Xc_te)[:,1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"정확도:\", accuracy_score(yc_te, pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(yc_te, proba))\n",
    "\n",
    "# ROC curve (단일 플롯)\n",
    "fpr, tpr, th = roc_curve(yc_te, proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"ROC\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"Logistic Regression ROC\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332da15c",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 전처리: 스케일링/정규화 & 원-핫 인코딩\n",
    "\n",
    "- Normalization vs Standardization  \n",
    "- `MinMaxScaler`, `StandardScaler`  \n",
    "- `OneHotEncoder` / `pandas.get_dummies`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6240ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "# 연속형 변수 스케일링\n",
    "X_demo = np.array([[1.0, 10.0],\n",
    "                   [2.0, 20.0],\n",
    "                   [3.0, 40.0]], dtype=float)\n",
    "\n",
    "print(\"원본:\\n\", X_demo)\n",
    "\n",
    "minmax = MinMaxScaler().fit_transform(X_demo)\n",
    "print(\"MinMaxScaler:\\n\", minmax)\n",
    "\n",
    "standard = StandardScaler().fit_transform(X_demo)\n",
    "print(\"StandardScaler:\\n\", standard)\n",
    "\n",
    "# 범주형 변수 One-hot\n",
    "df = pd.DataFrame({\n",
    "    \"city\": [\"Seoul\",\"Busan\",\"Seoul\",\"Daegu\"],\n",
    "    \"grade\": [\"A\",\"B\",\"A\",\"C\"],\n",
    "    \"value\": [10,20,15,12]\n",
    "})\n",
    "print(\"\\n원본 DF:\\n\", df)\n",
    "\n",
    "# pandas.get_dummies\n",
    "dummies = pd.get_dummies(df, columns=[\"city\",\"grade\"], drop_first=False)\n",
    "print(\"\\npandas.get_dummies 결과:\\n\", dummies)\n",
    "\n",
    "# sklearn OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "X_cat = enc.fit_transform(df[[\"city\",\"grade\"]])\n",
    "print(\"\\nOneHotEncoder 결과 shape:\", X_cat.shape)\n",
    "print(\"카테고리:\", enc.categories_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9ecb5",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Bias–Variance Trade-off 시각화\n",
    "\n",
    "- 다항 차수에 따른 **훈련/검증 MSE**를 비교하여 과소적합/과적합 양상을 관찰합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 데이터 생성 (노이즈 포함)\n",
    "rng = np.random.default_rng(0)\n",
    "X = np.linspace(-3, 3, 200).reshape(-1,1)\n",
    "y_true = np.sin(X).ravel()\n",
    "y = y_true + rng.normal(0, 0.2, size=y_true.shape)\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "degrees = range(1, 16)\n",
    "tr_errors, va_errors = [], []\n",
    "\n",
    "for d in degrees:\n",
    "    pf = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    Xtr_d = pf.fit_transform(X_tr)\n",
    "    Xva_d = pf.transform(X_va)\n",
    "    model = LinearRegression().fit(Xtr_d, y_tr)\n",
    "    tr_errors.append(mean_squared_error(y_tr, model.predict(Xtr_d)))\n",
    "    va_errors.append(mean_squared_error(y_va, model.predict(Xva_d)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(degrees, tr_errors, marker=\"o\", label=\"train MSE\")\n",
    "plt.plot(degrees, va_errors, marker=\"s\", label=\"valid MSE\")\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Bias–Variance trade-off (degree vs MSE)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726139e",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 정규화(Regularization): L1 vs L2\n",
    "\n",
    "- Ridge(L2), Lasso(L1) 비교: 계수 규모/희소성, 성능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b68231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=300, n_features=20, n_informative=5, noise=10.0, random_state=42)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "ols = LinearRegression().fit(X_tr, y_tr)\n",
    "ridge = Ridge(alpha=10.0, random_state=42).fit(X_tr, y_tr)\n",
    "lasso = Lasso(alpha=0.1, random_state=42, max_iter=10000).fit(X_tr, y_tr)\n",
    "\n",
    "for name, model in [(\"OLS\", ols), (\"Ridge\", ridge), (\"Lasso\", lasso)]:\n",
    "    mse = mean_squared_error(y_te, model.predict(X_te))\n",
    "    nnz = np.sum(model.coef_ != 0)\n",
    "    print(f\"{name:>5} | MSE={mse:.3f} | non-zero coeffs={nnz:2d}\")\n",
    "    # 일부 계수 출력\n",
    "print(\"OLS coef (앞 10개):\", np.round(ols.coef_[:10], 3))\n",
    "print(\"Ridge coef (앞 10개):\", np.round(ridge.coef_[:10], 3))\n",
    "print(\"Lasso coef (앞 10개):\", np.round(lasso.coef_[:10], 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d7c55",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Grid Search 실습\n",
    "\n",
    "- 파이프라인: `StandardScaler` + `Ridge`  \n",
    "- 그리드 서치로 `alpha` 및 정규화 전처리 여부 탐색\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=300, n_features=30, n_informative=6, noise=15.0, random_state=0)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", Ridge())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"scaler\": [StandardScaler(), \"passthrough\"],\n",
    "    \"ridge__alpha\": [0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    cv=kf,\n",
    "    n_jobs=None\n",
    ")\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"최적 파라미터:\", grid.best_params_)\n",
    "print(\"CV 최고 점수(음의 MSE):\", grid.best_score_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "print(\"테스트 점수 예시(재사용 데이터):\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0abe98",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✍️ 실습 과제 체크리스트\n",
    "\n",
    "1. **수치 미분** 함수로 `g(x) = sin(x) + 0.1x^3`의 도함수 값을 x ∈ {-2,0,2}에서 근사하고, 해석적 도함수와 비교하세요.  \n",
    "2. 단변량 회귀에서 **다항 차수**를 1, 3, 5로 바꾸어 **MSE**를 비교하고 그래프를 겹쳐 그리세요.  \n",
    "3. 로지스틱 회귀에서 **임계값(threshold)**을 0.3, 0.5, 0.7로 바꿔 **정확도/정밀도/재현율**을 비교하세요.  \n",
    "4. 스케일링 없이 Ridge를 학습한 경우와 **StandardScaler**를 적용한 경우의 성능 차이를 비교하세요.  \n",
    "5. Lasso의 `alpha`를 {0.01, 0.1, 1.0}로 바꾸며 **희소성(0이 아닌 계수 개수)** 변화를 관찰하세요.  \n",
    "6. GridSearchCV로 `PolynomialFeatures + LinearRegression` 파이프라인에서 `degree`를 {1,2,3,4,5}로 탐색하여 **최적 차수**를 찾으세요.\n",
    "\n",
    "> 필요 시 셀을 복사하여 자유롭게 실험하세요.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
