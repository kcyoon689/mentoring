<!DOCTYPE html><html lang="ko"><head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>머신러닝 알고리즘 심층 분석: 결정 트리부터 PCA까지</title>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<style>.container {
    max-width: 800px;
    margin: 0 auto;
    padding: 24px 40px;
    background-color: #fff;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    }
.chart-container {
    position: relative;
    margin: 3em auto;
    max-width: 700px;
    min-height: 200px;
    max-height: 400px;
    width: 100%;
    height: auto;
    overflow: visible;
    aspect-ratio: 7/5;}
h5 {
    font-size: 16px;
    }
body {
    font-family: "Georgia", "serif", "Apple SD Gothic Neo", "Malgun Gothic", sans-serif;
    line-height: 1.8;
    font-size: 16px;
    color: #333;
    background-color: #fdfdfd;
    margin: 0 24px 0 24px;
    padding: 0;
    max-width: None;
    }
h1, h2, h3, h4 {
    font-family: "Helvetica Neue", "Helvetica", "Arial", sans-serif;
    color: #1a1a1a;
    line-height: 1.3;
    margin-top: 24px;
    margin-bottom: 20px;
    font-size: 28px;
    }
h1 {
    font-size: 28px;
    text-align: center;
    border-bottom: 2px solid #eee;
    padding-bottom: 0.5em;
    margin-top: 24px;
    margin-bottom: 20px;
    }
h2 {
    font-size: 22px;
    border-bottom: 1px solid #eee;
    padding-bottom: 0.4em;
    }
h3 {
    font-size: 20px;
    }
h4 {
    font-size: 18px;
    }
p {
    margin-bottom: 1.2em;
    }
a {
    color: #07c;
    text-decoration: none;
    }
a:hover {
    text-decoration: underline;
    }
code {
    font-family: "Menlo", "Monaco", "Courier New", monospace;
    background-color: #f5f5f5;
    padding: 2px 6px;
    border-radius: 4px;
    font-size: 0.85em;
    }
pre {
    background-color: #f5f5f5;
    padding: 1em;
    border-radius: 4px;
    overflow-x: auto;
    font-size: 0.85em;
    line-height: 1.5;
    }
blockquote {
    border-left: 4px solid #ccc;
    padding-left: 1em;
    margin-left: 0;
    color: #666;
    font-style: italic;
    }
table {
    width: 100%;
    border-collapse: collapse;
    margin: 2em 0;
    font-size: 0.95em;
    }
th, td {
    border: 1px solid #ddd;
    padding: 12px;
    text-align: left;
    }
th {
    background-color: #f9f9f9;
    font-weight: bold;
    }
figure {
    margin: 2em 0;
    text-align: center;
    }
figcaption {
    font-size: 0.9em;
    color: #666;
    margin-top: 0.8em;
    font-style: italic;
    }
img {
    max-width: 100%;
    height: auto;
    border-radius: 8px;
    display: block;
    overflow: hidden;
    max-height: 280px;
    margin: 1em auto;
    }
.toc {
    background-color: #f9f9f9;
    border: 1px solid #e0e0e0;
    padding: 1.5em 2em;
    border-radius: 8px;
    margin-bottom: 3em;
    }
.toc h2 {
    font-size: 1.5em;
    border-bottom: none;
    margin-top: 0;
    }
.toc ul {
    list-style-type: none;
    padding-left: 0;
    }
.toc ul ul {
    padding-left: 2em;
    }
.toc li {
    margin-bottom: 0.5em;
    }
.key-points {
    background-color: #eef7ff;
    border: 1px solid #cce4ff;
    padding: 1.5em 2em;
    border-radius: 8px;
    margin: 2em 0;
    }
.key-points h3 {
    margin-top: 0;
    color: #00529b;
    }
        .chart-container canvas {
            width: 100% !important;
            height: 100% !important;
            object-fit: contain;
}

@media only screen and (max-device-width: 768px) {
            body {
                padding: 0;
                margin: 0;
                font-family: PingFang SC;
                font-size: 15px;
                line-height: 1.5;
            }

            .container {
                padding: 0;
                margin: 16px 20px 30px;
                box-shadow: none;
            }

            h1,
            h2,
            h3,
            h4 {
                font-family: PingFang SC;
            }

            h1 {
                font-size: 1.87em;
                line-height: 1.6;
                margin-bottom: 0.5em;
                text-align: center;
            }

            h2 {
                font-size: 1.6em;
                font-weight: 600;
                margin-top: 1.3em;
                margin-bottom: 0.8em;
                border-bottom: 1px solid #eee;
                padding-bottom: 0.5em;
            }

            h3 {
                font-size: 1.2em;
                font-weight: 600;
                margin-top: 1em;
                margin-bottom: 0.6em;
            }

            h4 {
                font-size: 1.1em;
                font-weight: 500;
                margin-top: 1em;
                margin-bottom: 0.5em;
                font-style: normal;
            }

            h5 {
                font-size: 1em;
                font-weight: 500;
                margin-bottom: 1.2em;
            }

            ul,
            ol {
                font-size: 1em; /* Equivalent to 17.6px if base is 16px */
                font-weight: 400;
                margin-bottom: 1.2em;
                line-height: 1.8;
            }

            p {
                font-size: 1em;
                line-height: 1.8; /* Equivalent to 17.6px if base is 16px */
                font-weight: 400;
                margin-top: 0.8em;
                margin-bottom: 0.8em;
            }

            blockquote {
                padding: 1em 1.2em;

            p {
                margin: 0;
            }
        }

        figcaption {
            margin-top: 0.5em;
            font-size: 0.8em; /* Equivalent to 17.6px if base is 16px */
            font-weight: 400;
            text-align: center;
            font-style: normal;
            color: #7F8896;
        }

        img {
            display: block;
            overflow: hidden;
            max-width: 100%;
            max-height: 335px;
            margin: 1em auto;
            border-radius: 8px;
        }
        }</style>
<link rel="stylesheet" href="https://static.skywork.ai/fe/skywork-site-assets/styles/doc_reference_style.css?v=1956373719124127744"/></head>
<body>
<div class="container">
<h1 id="section-1">머신러닝 알고리즘 심층 분석: 결정 트리부터 PCA까지</h1>
<nav class="toc" id="table-of-contents">
<h2 id="section-1">목차</h2>
<ul>
<li><a href="#intro">도입: 왜 우리는 머신러닝 알고리즘의 내부를 들여다봐야 하는가?</a></li>
<li><a href="#decision-tree">결정 트리 (Decision Tree): 직관적인 규칙의 시작</a></li>
<li><a href="#dt-core-principles">결정 트리의 핵심 원리: 불순도와 정보 이득</a>
<ul>
<li><a href="#section-gini-impurity">지니 불순도 (Gini Impurity) 심층 분석</a></li>
<li><a href="#section-information-gain">정보 이득 (Information Gain)</a></li>
</ul>
</li>
<li><a href="#feature-importance">결정 트리의 성능 평가: 속성 중요도 (Feature Importance)</a></li>
<li><a href="#ensemble-learning">앙상블 학습 (Ensemble Learning): 집단 지성의 힘</a></li>
<li><a href="#random-forest">랜덤 포레스트 (Random Forest): 배깅과 무작위성으로 과적합을 막다</a></li>
<li><a href="#adaboost">에이다부스트 (AdaBoost): 약한 학습기의 순차적 개선</a></li>
<li><a href="#rf-vs-adaboost">랜덤 포레스트 vs. 에이다부스트: 핵심 차이점 비교</a></li>
<li><a href="#dimensionality-reduction">차원 축소 (Dimensionality Reduction): 복잡한 데이터를 단순하게</a></li>
<li><a href="#pca">주성분 분석 (PCA): 데이터의 핵심을 꿰뚫는 선형 변환</a></li>
<li><a href="#beyond-pca">PCA를 넘어서: 다양한 차원 축소 기법들</a>
<ul>
<li><a href="#section-lda">선형 판별 분석 (LDA, Linear Discriminant Analysis)</a></li>
<li><a href="#section-tsne">t-SNE (t-Distributed Stochastic Neighbor Embedding)</a></li>
<li><a href="#section-autoencoders">오토인코더 (Autoencoders)</a></li>
</ul>
</li>
<li><a href="#conclusion">결론: 알고리즘을 이해하고 현명하게 선택하기</a></li>
</ul>
</nav>
<section id="intro">
<h2 id="section-2">도입: 왜 우리는 머신러닝 알고리즘의 내부를 들여다봐야 하는가?</h2>
<p>현대의 데이터 과학 시대에서 `scikit-learn`이나 `TensorFlow`와 같은 강력한 라이브러리는 단 몇 줄의 코드로 복잡한 머신러닝 모델을 구현하는 것을 가능하게 했다. 이러한 편리함은 혁신을 가속화했지만, 동시에 알고리즘을 &#39;블랙박스&#39;처럼 취급하는 경향을 낳기도 했다. 단순히 라이브러리를 호출하여 결과를 얻는 것에 만족한다면, 우리는 모델이 왜 특정 예측을 했는지, 어떤 요인이 결과에 결정적인 영향을 미쳤는지, 그리고 모델의 잠재적인 한계와 편향은 무엇인지 알 수 없다.</p>
<p>알고리즘의 내부 작동 원리를 이해하는 것은 단순히 학문적 호기심을 충족시키는 것을 넘어, 실질적인 문제 해결 능력을 배양하는 데 필수적이다. 모델의 해석 가능성(Interpretability)을 확보하면 예측 결과에 대한 신뢰를 높이고 비즈니스 의사결정에 더 깊이 관여할 수 있다. 또한, 성능이 기대에 미치지 못할 때 어떤 하이퍼파라미터를 조정해야 할지, 데이터 전처리를 어떻게 개선해야 할지에 대한 단서를 얻을 수 있다. 궁극적으로는 주어진 문제와 데이터의 특성에 가장 적합한 알고리즘을 &#39;감&#39;이 아닌 &#39;논리&#39;에 근거하여 선택하는 능력을 기르게 된다.</p>
<p>이 글은 이러한 문제의식에서 출발한다. 우리는 가장 직관적인 모델 중 하나인 **결정 트리(Decision Tree)**에서 여정을 시작하여, 그 한계를 극복하기 위해 집단 지성을 활용하는 **앙상블(Ensemble) 기법**인 **랜덤 포레스트(Random Forest)**와 **에이다부스트(AdaBoost)**를 탐구할 것이다. 나아가, 데이터의 본질적인 구조를 꿰뚫어 복잡성을 줄이는 **차원 축소(Dimensionality Reduction)**의 세계로 나아가, 그 대표적인 기법인 **주성분 분석(PCA)**과 그 외 다양한 방법들을 심층적으로 분석할 것이다. 이 여정을 통해 독자들이 각 알고리즘의 철학과 장단점을 명확히 이해하고, 실제 문제 해결을 위한 강력하고 신뢰할 수 있는 도구 상자를 구축하는 것을 목표로 한다.</p>
</section>
<section id="decision-tree">
<h2 id="section-3">결정 트리 (Decision Tree): 직관적인 규칙의 시작</h2>
<p>결정 트리(Decision Tree)는 머신러닝의 여러 알고리즘 중 가장 직관적이고 이해하기 쉬운 모델 중 하나이다. 그 작동 방식은 마치 &#39;스무고개&#39; 게임과 유사하다. <a href="https://lcyking.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%ACDecision-Tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98" target="_blank">데이터에 대한 연속적인 질문(if/else 규칙)을 통해</a> 데이터를 특정 그룹으로 분류하거나 특정 값을 예측하는 지도 학습(Supervised Learning) 모델이다. 예를 들어, &#34;어떤 회사의 입사 제의를 수락할 것인가?&#34;라는 문제를 결정 트리로 모델링한다면, &#34;연봉이 5,000만 원 이상인가?&#34;, &#34;통근 거리가 1시간 이내인가?&#34;와 같은 질문들을 연달아 던져 최종적으로 &#39;수락&#39; 또는 &#39;거절&#39;을 결정하는 구조를 만들 수 있다.</p>
<h3 id="section-3-1">구조와 용어</h3>
<p>결정 트리는 이름에서 알 수 있듯이 나무(Tree) 구조를 가진다. 이 구조를 이해하기 위해 몇 가지 핵심 용어를 알아둘 필요가 있다.</p>
<ul>
<li><strong>루트 노드 (Root Node):</strong> 의사결정 과정이 시작되는 가장 위쪽의 노드. 전체 데이터셋을 포함하며, 여기서 첫 번째 질문이 시작된다.</li>
<li><strong>내부 노드 (Internal Node):</strong> 데이터를 추가적으로 분기(split)하기 위한 질문, 즉 규칙을 담고 있는 중간 단계의 노드들이다. 하나의 부모 노드와 두 개 이상의 자식 노드를 가진다.</li>
<li><strong>리프 노드 (Leaf Node / Terminal Node):</strong> 더 이상 분기되지 않는 나무의 가장 마지막 노드. 이 노드에서 최종적인 분류 결과(예: &#39;setosa&#39;, &#39;versicolor&#39;)나 회귀 예측값(예: &#39;주택 가격 3억 원&#39;)이 결정된다.</li>
</ul>
<h3 id="section-3-2">활용 분야</h3>
<p>결정 트리의 큰 장점 중 하나는 유연성이다. <a href="https://www.ibm.com/kr-ko/think/topics/decision-trees" target="_blank">하나의 알고리즘으로 분류(Classification)와 회귀(Regression) 문제 모두에 적용</a>할 수 있다. 예를 들어, 고객 데이터를 기반으로 이탈 여부(&#39;이탈&#39;/&#39;유지&#39;)를 예측하는 것은 분류 문제이며, 고객의 나이, 소득, 구매 기록 등을 바탕으로 연간 구매액을 예측하는 것은 회귀 문제이다. 결정 트리는 이 두 가지 유형의 문제를 모두 효과적으로 다룰 수 있다.</p>
<h3 id="section-3-3">기원</h3>
<p>현대의 결정 트리 알고리즘은 1984년 Leo Breiman 등에 의해 개발된 **CART(Classification And Regression Tree)** 알고리즘에 깊은 뿌리를 두고 있다. <a href="https://meme2515.github.io/machine_learning/decision_tree/" target="_blank">CART는 오늘날 널리 사용되는 XGBoost, LightGBM과 같은 강력한 앙상블 모델의 기본 구성 요소(base learner)로 활용</a>되며, 머신러닝 발전사에 중요한 이정표를 남겼다. 이처럼 결정 트리는 그 자체로도 강력한 모델이지만, 더 복잡하고 정교한 알고리즘의 근간을 이룬다는 점에서 그 중요성이 더욱 크다고 할 수 있다.</p>
</section>
<section id="dt-core-principles">
<h2 id="section-4">결정 트리의 핵심 원리: 불순도와 정보 이득</h2>
<p>결정 트리가 &#39;스무고개&#39;와 같다고 했다. 그렇다면 트리는 어떻게 스스로 &#39;최적의&#39; 질문을 찾아내는 것일까? 예를 들어, 붓꽃 품종을 분류하는 문제에서 &#34;꽃잎의 길이가 5cm보다 긴가?&#34;와 &#34;꽃받침의 너비가 3cm보다 넓은가?&#34; 중 어떤 질문이 더 효과적인지를 어떻게 판단할까? 이 질문에 대한 답은 바로 **불순도(Impurity)**와 **정보 이득(Information Gain)** 개념에 있다.</p>
<p>결정 트리의 학습 목표는 각 분기(질문)를 통해 생성된 자식 노드들이 최대한 **순수한(pure)** 상태가 되도록 하는 것이다. 여기서 &#39;순수하다&#39;는 것은 <a href="https://lcyking.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%ACDecision-Tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98" target="_blank">하나의 노드에 속한 데이터들이 가능한 한 동일한 클래스로 구성되는 것</a>을 의미한다. 예를 들어, 어떤 노드에 붓꽃 데이터 50개가 있는데 모두 &#39;Setosa&#39; 품종이라면 이 노드는 순도 100%의 완벽히 순수한 노드이다. 반면, &#39;Setosa&#39; 25개와 &#39;Versicolor&#39; 25개가 섞여 있다면 이 노드는 매우 불순한 상태라고 할 수 있다. 결정 트리는 이러한 불순도를 정량적으로 측정하고, 이를 가장 많이 줄여주는 질문을 찾아 학습을 진행한다.</p>
<h3 id="section-gini-impurity">지니 불순도 (Gini Impurity) 심층 분석</h3>
<p>불순도를 측정하는 대표적인 지표가 바로 **지니 불순도(Gini Impurity)**이다. 지니 불순도는 <a href="https://datasciencebeehive.tistory.com/84" target="_blank">특정 노드에 속한 데이터들이 얼마나 다른 클래스들로 섞여 있는지를 측정하는 지표</a>로, 0부터 1 사이의 값을 가진다. 통계적으로는 해당 노드에서 무작위로 하나의 데이터를 뽑아 그 데이터의 클래스를 예측할 때 틀릴 확률로 해석할 수 있다.</p>
<h4>계산 공식과 의미</h4>
<p>지니 불순도는 다음 공식으로 계산된다. 여기서 `p_k`는 해당 노드에서 클래스 `k`에 속하는 샘플의 비율을 의미한다.</p>
<blockquote>
<p><strong>Gini = 1 - Σ (p<sub>k</sub>)<sup>2</sup></strong></p>
</blockquote>
<ul>
<li><strong>지니 불순도 = 0</strong>: 노드가 완벽하게 순수한 상태. 모든 데이터가 단 하나의 클래스에 속한다. (예: p<sub>1</sub>=1, p<sub>2</sub>=0 이면, Gini = 1 - (1<sup>2</sup> + 0<sup>2</sup>) = 0)</li>
<li><strong>지니 불순도 = 0.5 (이진 분류의 경우)</strong>: 노드가 최악으로 불순한 상태. 데이터가 모든 클래스에 걸쳐 균등하게 분포한다. (예: p<sub>1</sub>=0.5, p<sub>2</sub>=0.5 이면, Gini = 1 - (0.5<sup>2</sup> + 0.5<sup>2</sup>) = 0.5)</li>
</ul>
<h4>직관적 예시</h4>
<p>과일 상자를 예로 들어 지니 불순도를 직관적으로 이해해보자.</p>
<ol>
<li><strong>완벽히 순수한 경우:</strong> 상자에 사과만 10개 들어있다. 이 상자에서 과일을 하나 꺼내 &#34;이건 사과야&#34;라고 예측하면 100% 맞춘다. 이 경우 사과의 비율(p<sub>사과</sub>)은 1.0이다.
                        <br/><code>Gini = 1 - (1.0)<sup>2</sup> = 0</code></li>
<li><strong>최악으로 불순한 경우:</strong> 상자에 사과 5개, 바나나 5개가 들어있다. 사과의 비율(p<sub>사과</sub>)과 바나나의 비율(p<sub>바나나</sub>)은 모두 0.5이다.
                        <br/><code>Gini = 1 - [(0.5)<sup>2</sup> + (0.5)<sup>2</sup>] = 1 - (0.25 + 0.25) = 0.5</code></li>
<li><strong>일반적인 경우:</strong> 상자에 사과 8개, 바나나 2개가 들어있다. 사과의 비율은 0.8, 바나나의 비율은 0.2이다.
                        <br/><code>Gini = 1 - [(0.8)<sup>2</sup> + (0.2)<sup>2</sup>] = 1 - (0.64 + 0.04) = 1 - 0.68 = 0.32</code></li>
</ol>
<p>이처럼 지니 불순도 값이 낮을수록 해당 노드의 데이터가 더 순수하다는 것을 의미한다. <a href="https://casa-de-feel.tistory.com/6" target="_blank">결정 트리는 지니 불순도가 가장 낮아지는 방향으로 속성을 선택하고 트리를 분할</a>해 나간다.</p>
<h4>연속형 변수 처리</h4>
<p>온도, 나이, 길이와 같은 연속형 변수는 어떻게 분기 기준으로 삼을까? 무한한 경계값을 모두 테스트할 수는 없다. CART 알고리즘은 이를 효율적으로 처리한다. 먼저, <a href="https://velog.io/@lsy0476/%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-Decision-Tree" target="_blank">해당 변수 값을 기준으로 데이터를 정렬</a>한다. 그 다음, 클래스(정답)가 바뀌는 지점들을 찾고, 그 경계값들의 평균값을 분기 후보로 삼는다. 각 후보 경계값에 대해 지니 불순도를 계산하여, 불순도를 가장 많이 낮추는 최적의 분할 지점을 찾아낸다.</p>
<figure>
<figcaption>연속형 변수(체온)를 정렬하고 각 가능한 분기점에 대해 지니 불순도를 계산하여 최적의 분할 기준을 찾는 과정</figcaption>
</figure>
<h3 id="section-information-gain">정보 이득 (Information Gain)</h3>
<p>결정 트리가 최적의 질문을 선택하는 기준은 바로 **정보 이득**을 최대화하는 것이다. 정보 이득이란, <a href="https://lcyking.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%ACDecision-Tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98" target="_blank">특정 속성을 기준으로 데이터를 분기했을 때 불순도가 얼마나 감소했는지를 나타내는 척도</a>이다. 정보 이득이 크다는 것은 그만큼 해당 분기가 데이터를 더 &#39;잘&#39; 나누었다는 의미이다.</p>
<p>정보 이득은 다음과 같이 계산된다. 여기서 자식 노드의 불순도는 각 노드에 속한 데이터 수에 따라 가중 평균을 적용한다.</p>
<blockquote>
<p><strong>정보 이득 = 부모 노드의 불순도 - (자식 노드들의 가중 평균 불순도)</strong></p>
</blockquote>
<p>결정 트리 학습 알고리즘(예: CART)은 가능한 모든 속성과 분기점에 대해 정보 이득을 계산하고, 이 값이 최대가 되는 조합을 선택하여 노드를 분할한다. 이 과정을 리프 노드의 불순도가 0이 되거나, 더 이상 분할할 수 없는 등의 정지 조건에 도달할 때까지 재귀적으로 반복한다.</p>
</section>
<section id="feature-importance">
<h2 id="section-5">결정 트리의 성능 평가: 속성 중요도 (Feature Importance)</h2>
<p>결정 트리 모델을 만들고 나면, 우리는 &#34;그래서 어떤 변수가 예측에 가장 큰 영향을 미쳤는가?&#34;라는 질문을 던지게 된다. 모델의 예측 결과를 신뢰하고 해석하기 위해 이 질문에 답하는 것은 매우 중요하다. **속성 중요도(Feature Importance)**는 바로 이 질문에 대한 답을 제공하는 지표로, <a href="https://blog.naver.com/fbfbf1/222431450205" target="_blank">모델이 예측을 수행하는 과정에서 각 속성(feature)이 얼마나 중요하게 사용되었는지를 정량적으로 평가</a>한다.</p>
<h3 id="section-5-1">계산 원리: 불순도 감소량의 총합</h3>
<p>속성 중요도는 직관적인 아이디어에 기반한다. &#34;중요한 속성일수록 데이터를 순수하게 만드는 데 더 많이 기여했을 것이다.&#34; 즉, 특정 속성이 트리의 노드를 분기할 때마다 기여한 &#39;불순도 감소량(정보 이득)&#39;을 모두 합산하여 그 속성의 중요도를 계산한다.</p>
<p>Scikit-learn의 결정 트리에서 속성 중요도를 계산하는 과정은 다음과 같다 (<a href="https://stackoverflow.com/questions/49170296/scikit-learn-feature-importance-calculation-in-decision-trees" target="_blank">Gini Importance 또는 Mean Decrease in Impurity(MDI)</a>라고도 불린다):</p>
<ol>
<li><strong>노드 중요도 계산:</strong> 트리의 각 노드(루트 노드 제외)가 분기되면서 발생한 불순도 감소량을 계산한다. 이 값은 `(부모 노드의 샘플 수 / 전체 샘플 수) * (부모 노드 불순도 - 자식 노드들의 가중 평균 불순도)`와 같이 계산될 수 있다. 즉, 해당 분기가 전체 트리의 불순도를 얼마나 낮췄는지에 대한 기여도이다.</li>
<li><strong>속성별 중요도 합산:</strong> 트리 전체를 순회하며, 특정 속성(예: &#39;꽃잎 길이&#39;)이 분기 기준으로 사용된 모든 노드의 중요도를 찾아 모두 더한다.</li>
<li><strong>정규화:</strong> 이렇게 계산된 각 속성의 중요도 총합을 모든 속성의 중요도 총합으로 나누어, 전체 합이 1이 되도록 정규화한다. 이 값은 0과 1 사이의 값을 가지며, 0은 전혀 사용되지 않았음을, 1에 가까울수록 완벽하게 타겟을 예측하는 데 기여했음을 의미한다.</li>
</ol>
<p>결론적으로, 속성 중요도는 <a href="https://velog.io/@lsy0476/%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-Decision-Tree" target="_blank">해당 속성이 트리 전체의 불순도를 낮추는 데 평균적으로 얼마나 기여했는가</a>를 나타내는 값이다.</p>
<h3 id="section-5-2">실용적 해석과 시각화</h3>
<p>Python의 `scikit-learn` 라이브러리를 사용하면, 학습된 결정 트리 모델 객체의 `feature_importances_` 속성을 통해 각 속성의 중요도를 쉽게 확인할 수 있다. 이 숫자 배열만으로는 직관적인 파악이 어려우므로, 보통 막대그래프(Bar chart)로 시각화하여 어떤 속성이 모델 결정에 핵심적인 역할을 했는지 한눈에 파악한다.</p>
<p>아래 차트는 가상의 데이터셋으로 학습한 모델의 속성 중요도를 시각화한 예시이다. 이 차트를 통해 &#39;feature 1&#39;이 예측에 가장 결정적인 영향을 미쳤으며, 그 다음으로 &#39;feature 0&#39;과 &#39;feature 2&#39;가 중요하게 작용했음을 명확히 알 수 있다. 반면, 나머지 속성들은 모델 성능에 거의 기여하지 못했음을 보여준다. 이러한 정보는 불필요한 속성을 제거하는 특성 선택(Feature Selection) 과정이나, 비즈니스 의사결정자에게 모델의 작동 방식을 설명하는 데 매우 유용하게 사용될 수 있다.</p>
<div class="chart-container" id="canvas-parent-1">
<canvas id="featureImportanceChart"></canvas>
</div>
</section>
<section id="ensemble-learning">
<h2 id="section-6">앙상블 학습 (Ensemble Learning): 집단 지성의 힘</h2>
<p>&#34;백지장도 맞들면 낫다&#34;는 속담처럼, 머신러닝에서도 &#34;하나의 강력한 모델보다 여러 개의 평범한 모델이 더 낫다&#34;는 아이디어가 존재한다. 이것이 바로 **앙상블 학습(Ensemble Learning)**의 핵심 철학이다. 앙상블 학습은 <a href="https://day-to-day.tistory.com/35" target="_blank">여러 개의 개별 모델(약한 학습기, Weak Learner)을 조합하여 하나의 강력한 최종 모델(강한 학습기, Strong Learner)을 만드는 기법</a>이다.</p>
<p>특히 결정 트리는 구조가 간단하고 해석이 용이하지만, 데이터의 작은 변화에도 구조가 크게 바뀔 수 있어 불안정하며, 너무 깊게 학습할 경우 훈련 데이터에만 과도하게 최적화되는 **과적합(Overfitting)** 문제가 발생하기 쉽다. 앙상블 기법은 이러한 단일 모델의 한계를 극복하기 위한 매우 효과적인 대안이다. <a href="https://www.koreascience.kr/article/JAKO202211954995024.pdf" target="_blank">여러 모델의 예측을 종합함으로써, 개별 모델이 가질 수 있는 편향(Bias)과 분산(Variance)을 동시에 줄여</a> 모델의 일반화 성능을 크게 향상시킬 수 있다.</p>
<h3 id="section-6-1">주요 기법 소개</h3>
<p>앙상블 기법은 약한 학습기들을 결합하는 방식에 따라 크게 두 가지로 나뉜다.</p>
<ul>
<li><strong>배깅 (Bagging - Bootstrap Aggregating):</strong> 여러 학습기가 서로 독립적으로, **병렬적**으로 학습을 진행한다. 각 학습기는 원본 데이터에서 무작위로 복원 추출한 서브 데이터셋(Bootstrap Sample)으로 학습하여 다양성을 확보한다. 대표적인 예로 **랜덤 포레스트**가 있다.</li>
<li><strong>부스팅 (Boosting):</strong> 여러 학습기가 **순차적**으로 학습을 진행한다. 앞선 학습기가 예측에 실패한(오류가 큰) 데이터에 가중치를 부여하여, 다음 학습기가 이 어려운 문제에 더 집중하도록 만든다. 대표적인 예로 **에이다부스트**, **그래디언트 부스팅**이 있다.</li>
</ul>
<figure>
<figcaption>결정 트리를 기반으로 하는 다양한 앙상블 학습 기법의 관계도</figcaption>
</figure>
</section>
<section id="random-forest">
<h2 id="section-7">랜덤 포레스트 (Random Forest): 배깅과 무작위성으로 과적합을 막다</h2>
<p>랜덤 포레스트는 이름 그대로 &#39;무작위로 생성된 숲&#39;이다. 여기서 &#39;나무&#39;는 결정 트리를 의미하며, 수많은 결정 트리들이 모여 하나의 숲(앙상블)을 이룬다. <a href="https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8" target="_blank">결정 트리를 기반으로 하는 대표적인 배깅(Bagging) 방식의 앙상블 알고리즘</a>으로, 분류와 회귀 문제 모두에서 매우 뛰어난 성능을 보여 머신러닝 대회나 실제 현업에서 널리 사용된다.</p>
<h3 id="section-7-1">작동 방식의 두 가지 핵심: 무작위성</h3>
<p>랜덤 포레스트의 강력함은 &#39;무작위성(Randomness)&#39;을 두 단계에 걸쳐 주입하여 개별 트리들의 예측을 다양하게 만들고, 이들의 상관관계를 낮추는 데서 비롯된다. <a href="https://www.ibm.com/kr-ko/think/topics/random-forest" target="_blank">이것이 단일 결정 트리와 랜덤 포레스트의 핵심적인 차이점이다.</a></p>
<ol>
<li><strong>데이터 무작위성 (배깅, Bagging):</strong> 첫 번째 무작위성은 데이터 샘플링 과정에서 발생한다. 원본 훈련 데이터셋에서 중복을 허용하여(복원 추출) 무작위로 여러 개의 서브 데이터셋을 만든다. 이를 **부트스트랩 샘플(Bootstrap Sample)**이라고 한다. 숲을 구성하는 각 결정 트리는 이렇게 생성된 서로 다른 서브 데이터셋으로 학습한다. 이 과정을 통해 각 트리는 데이터의 서로 다른 측면을 학습하게 되어 다양성이 확보된다.</li>
<li><strong>속성 무작위성 (Feature Randomness):</strong> 두 번째 무작위성은 트리의 각 노드를 분할할 때 적용된다. 일반적인 결정 트리는 모든 속성을 고려하여 최적의 분할 기준을 찾지만, 랜덤 포레스트는 <a href="blog-ko.superb-ai.com/3-minute-algorithm-random-forest/" target="_blank">전체 속성 중 일부(보통 전체 속성 개수의 제곱근)를 무작위로 선택하고, 그 선택된 속성들 내에서만 최적의 분할 기준을 찾는다.</a> 이 방식은 특정 예측력이 강한 소수의 속성에만 모델이 의존하는 현상을 방지한다. 예를 들어, 한 속성이 매우 강력하다면, 배깅만으로는 모든 트리가 그 속성을 최상위 노드에서 분기 기준으로 사용할 가능성이 높다. 이는 트리 간의 상관관계를 높여 앙상블의 효과를 감소시킨다. 속성 무작위성은 이러한 문제를 해결하여 트리들이 더욱 독립적이고 다양해지도록 만든다.</li>
</ol>
<h3 id="section-7-2">최종 예측</h3>
<p>수백 개의 다양한 트리들이 각자의 예측을 내놓으면, 랜덤 포레스트는 이 결과들을 민주적으로 종합한다.</p>
<ul>
<li><strong>분류 문제:</strong> 모든 트리가 예측한 클래스 중 가장 많이 나온 클래스를 최종 결과로 선택한다. 이를 **다수결 투표(Majority Voting)**라고 한다.</li>
<li><strong>회귀 문제:</strong> 모든 트리가 예측한 값들의 **평균(Average)**을 최종 예측값으로 사용한다.</li>
</ul>
<h3 id="section-7-3">장점과 단점</h3>
<div class="key-points">
<h3 id="section-7-4">핵심 요약</h3>
<p><strong>장점:</strong></p>
<ul>
<li><strong>강력한 과적합 방지:</strong> 두 가지 무작위성 덕분에 단일 결정 트리의 가장 큰 단점인 과적합 문제에 매우 강하다. </li>
<li><strong>높은 정확도:</strong> 일반적으로 대부분의 문제에서 매우 높은 예측 성능을 보인다.</li>
<li><strong>사용 용이성:</strong> 특별한 하이퍼파라미터 튜닝 없이도 준수한 성능을 내며, 데이터 스케일링 등 전처리에 덜 민감하다.</li>
<li><strong>결측치 처리:</strong> 결측치가 있는 데이터에도 상대적으로 강건한 성능을 유지한다.</li>
</ul>
<p><strong>단점:</strong></p>
<ul>
<li><strong>속도와 메모리:</strong> <a href="https://bommbom.tistory.com/entry/%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8Random-Forest-%EC%9E%A5%EB%8B%A8%EC%A0%90-%ED%8A%B9%EC%84%B1-%EC%A4%91%EC%9A%94%EB%8F%84" target="_blank">수백 개의 트리를 생성하고 학습해야 하므로</a> 단일 트리보다 학습 속도가 느리고 더 많은 메모리를 사용한다.</li>
<li><strong>해석의 어려움:</strong> 모델이 수많은 트리의 조합으로 이루어져 있어, 단일 결정 트리처럼 예측 과정을 명확하고 직관적으로 해석하기는 어렵다. (물론, 속성 중요도 등을 통해 전반적인 해석은 가능하다.)</li>
</ul>
</div>
</section>
<section id="adaboost">
<h2 id="section-8">에이다부스트 (AdaBoost): 약한 학습기의 순차적 개선</h2>
<p>에이다부스트(AdaBoost)는 **Adaptive Boosting**의 줄임말로, 랜덤 포레스트와 함께 앙상블 기법의 양대 산맥을 이루는 대표적인 부스팅(Boosting) 알고리즘이다. 랜덤 포레스트가 여러 학습기를 병렬적으로 학습시키는 &#39;민주적&#39; 방식이라면, 에이다부스트는 &#34;실수로부터 배운다&#34;는 철학을 바탕으로 약한 학습기들을 순차적으로 개선해나가는 &#39;엘리트적&#39; 방식이다.</p>
<h3 id="section-8-1">핵심 아이디어: &#34;실수로부터 배운다&#34;</h3>
<p>에이다부스트의 핵심은 <a href="https://bommbom.tistory.com/entry/Boosting-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-Adaboost-%EB%8F%99%EC%9E%91-%EC%9B%90%EB%A6%AC" target="_blank">이전 단계 학습기의 단점을 다음 단계 학습기가 보완하는 방식으로 작동</a>한다. 이 과정은 데이터 샘플에 &#39;가중치&#39;를 부여함으로써 이루어진다.</p>
<ol>
<li><strong>순차적 학습 시작:</strong> 먼저, 모든 훈련 데이터 샘플에 동일한 가중치를 부여하고, 첫 번째 약한 학습기(Weak Learner)를 학습시킨다. 에이다부스트의 약한 학습기는 보통 깊이가 1인 매우 단순한 결정 트리, 즉 **결정 그루터기(Decision Stump)**를 사용한다. </li>
<li><strong>가중치 재조정:</strong> 첫 번째 학습기가 예측한 결과를 실제 값과 비교한다. 이때, **잘못 분류된 데이터 샘플의 가중치는 높이고, 올바르게 분류된 샘플의 가중치는 낮춘다.** 이는 모델이 맞추기 어려워하는 &#39;까다로운&#39; 문제에 더 집중하도록 유도하는 과정이다.</li>
<li><strong>오류 보완 학습:</strong> 두 번째 약한 학습기는 이렇게 가중치가 재조정된 데이터를 기반으로 학습한다. 즉, 가중치가 높은 샘플들을 올바르게 분류하는 것을 더 중요한 목표로 삼게 된다.</li>
<li><strong>반복:</strong> 이 과정을 정해진 횟수(학습기 개수)만큼 반복한다. 각 단계의 학습기는 이전 단계까지의 누적된 오류를 보완하는 방향으로 학습을 진행한다.</li>
</ol>
<figure>
<figcaption>에이다부스트는 순차적으로 약한 학습기를 만들며, 이전 단계에서 잘못 분류된 데이터(크기가 커진 +,- 기호)에 가중치를 부여하여 다음 학습기가 오류를 보완하도록 학습한다</figcaption>
</figure>
<h3 id="section-8-2">최종 예측: 전문가에게 더 큰 발언권을</h3>
<p>모든 약한 학습기들의 학습이 끝나면, 최종 예측은 이들의 결과를 종합하여 이루어진다. 하지만 랜덤 포레스트처럼 모든 학습기가 동등한 한 표를 행사하는 것이 아니다. 에이다부스트는 <a href="https://helpingstar.github.io/ml/ensemble/" target="_blank">학습 과정에서 오류율이 낮았던, 즉 성능이 좋았던 학습기의 예측에 더 높은 가중치를 부여</a>한다. 이는 마치 여러 전문가의 의견을 취합할 때, 더 신뢰도 높은 전문가의 의견에 더 큰 비중을 두는 것과 같다. 이러한 **가중 투표(Weighted Voting)**를 통해 최종 결정을 내리므로, 더 정교하고 강력한 예측이 가능해진다.</p>
<h3 id="section-8-3">장점과 단점</h3>
<div class="key-points">
<h3 id="section-8-4">핵심 요약</h3>
<p><strong>장점:</strong></p>
<ul>
<li><strong>높은 예측 성능:</strong> 약한 학습기를 결합하여 매우 강력한 분류기를 만들 수 있으며, 일반적으로 높은 정확도를 보인다.</li>
<li><strong>구현의 용이성:</strong> 알고리즘이 비교적 간단하며, 랜덤 포레스트에 비해 튜닝할 하이퍼파라미터가 적은 편이다.</li>
<li><strong>과적합 경향 감소:</strong> 앙상블 모델의 특성상 단일 모델보다 과적합에 강한 경향을 보인다. </li>
</ul>
<p><strong>단점:</strong></p>
<ul>
<li><strong>노이즈 및 이상치에 민감:</strong> <a href="https://mozenworld.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8-%EC%86%8C%EA%B0%9C-5-%EC%97%90%EC%9D%B4%EB%8B%A4%EB%B6%80%EC%8A%A4%ED%8A%B8-AdaBoost" target="_blank">잘못 분류된 샘플에 집중하는 메커니즘 때문에</a> 데이터에 노이즈나 이상치가 포함되어 있을 경우, 모델 성능이 크게 저하될 수 있다.</li>
<li><strong>학습 속도:</strong> 학습 과정이 순차적으로 진행되므로, CPU 코어가 많아도 병렬 처리가 불가능하여 데이터가 크거나 학습기 수가 많을 경우 학습 시간이 길어질 수 있다. </li>
</ul>
</div>
</section>
<section id="rf-vs-adaboost">
<h2 id="section-9">랜덤 포레스트 vs. 에이다부스트: 핵심 차이점 비교</h2>
<p>랜덤 포레스트와 에이다부스트는 모두 결정 트리를 기반으로 한 강력한 앙상블 기법이지만, 그 철학과 작동 방식에는 명확한 차이가 있다. 어떤 상황에서 어떤 모델을 선택해야 할지 결정하기 위해서는 이 차이점을 명확히 이해하는 것이 중요하다. <a href="https://www.geeksforgeeks.org/machine-learning/differences-between-random-forest-and-adaboost/" target="_blank">두 알고리즘의 핵심적인 차이점</a>은 다음과 같이 요약할 수 있다.</p>
<table>
<thead>
<tr>
<th>구분</th>
<th>랜덤 포레스트 (Random Forest)</th>
<th>에이다부스트 (AdaBoost)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>학습 방식</strong></td>
<td><strong>병렬적 (Parallel)</strong>: 각 트리가 서로에게 영향을 주지 않고 독립적으로 학습한다. 병렬 처리에 용이하다.</td>
<td><strong>순차적 (Sequential)</strong>: 이전 트리가 잘못 분류한 샘플에 가중치를 부여하여 다음 트리가 학습한다. 순서가 중요하다.</td>
</tr>
<tr>
<td><strong>기본 학습기</strong></td>
<td>비교적 깊고 복잡한 결정 트리(Full-sized Decision Tree)를 사용하여 분산을 낮추는 데 집중한다.</td>
<td>깊이가 얕은 단순한 결정 트리, 주로 결정 그루터기(Decision Stump)를 사용하여 편향을 점진적으로 줄여나간다.</td>
</tr>
<tr>
<td><strong>데이터 샘플링</strong></td>
<td><strong>배깅 (Bagging)</strong>: 원본 데이터에서 중복을 허용한 무작위 샘플링(Bootstrap)을 통해 각 트리가 다른 데이터로 학습한다.</td>
<td><strong>가중치 재조정 (Re-weighting)</strong>: 전체 데이터를 사용하되, 잘못 분류된 샘플의 가중치를 높여 다음 학습기가 더 집중하도록 한다.</td>
</tr>
<tr>
<td><strong>최종 결정</strong></td>
<td><strong>동등한 투표 (Equal Vote)</strong>: 모든 트리가 동등하게 1표씩 행사하여 다수결 또는 평균으로 결정한다.</td>
<td><strong>가중 투표 (Weighted Vote)</strong>: <a href="https://itforfun.tistory.com/268" target="_blank">성능이 좋은(오류율이 낮은) 트리가 더 큰 영향력(가중치)</a>을 가지고 투표한다.</td>
</tr>
<tr>
<td><strong>주요 목표</strong></td>
<td><strong>분산 감소 (Variance Reduction)</strong>: 개별 트리들이 과적합되는 경향을 상쇄시켜 모델의 안정성을 높인다.</td>
<td><strong>편향 감소 (Bias Reduction)</strong>: 약한 학습기들을 결합하여 점진적으로 성능을 향상시켜 더 정확한 모델을 만든다.</td>
</tr>
<tr>
<td><strong>이상치 민감도</strong></td>
<td>여러 트리가 독립적으로 학습하고 결과를 평균내므로 이상치의 영향이 분산되어 상대적으로 덜 민감하다.</td>
<td>이상치를 잘못 분류할 경우 해당 샘플에 매우 높은 가중치가 부여되어 모델 전체가 왜곡될 수 있으므로 상대적으로 민감하다.</td>
</tr>
</tbody>
</table>
</section>
<section id="dimensionality-reduction">
<h2 id="section-10">차원 축소 (Dimensionality Reduction): 복잡한 데이터를 단순하게</h2>
<p>현대 데이터는 종종 수백, 수천 개가 넘는 속성(feature)을 포함한다. 예를 들어, 온라인 쇼핑몰의 고객 데이터는 나이, 성별, 지역과 같은 기본 정보 외에도 수백 가지 상품에 대한 클릭 여부, 구매 여부, 장바구니 추가 여부 등 엄청난 수의 변수를 가질 수 있다. 이렇게 데이터의 차원(속성의 수)이 증가할수록, 우리는 **&#39;차원의 저주(Curse of Dimensionality)&#39;**라는 문제에 직면하게 된다.</p>
<p><a href="https://encord.com/blog/dimentionality-reduction-techniques-machine-learning/" target="_blank">차원이 높아질수록 데이터 포인트 간의 거리는 기하급수적으로 멀어지고, 데이터를 표현하는 공간은 극도로 희소(sparse)해진다.</a> 이는 마치 넓은 우주 공간에 별들이 드문드문 흩어져 있는 것과 같다. 이러한 환경에서는 유의미한 패턴을 찾기 위해 훨씬 더 많은 데이터가 필요하며, 머신러닝 모델은 훈련 데이터에만 존재하는 우연한 패턴까지 학습하여 과적합되기 쉽다. 이것이 바로 차원 축소가 필요한 이유이다.</p>
<h3 id="section-10-1">차원 축소의 필요성</h3>
<ul>
<li><strong>성능 향상:</strong> 데이터에 포함된 노이즈나 관련 없는 속성을 제거하여 모델의 과적합을 방지하고, 새로운 데이터에 대한 일반화 성능을 높인다. </li>
<li><strong>계산 효율성:</strong> 학습 데이터의 크기를 줄여 모델의 학습 속도를 크게 향상시키고, 필요한 저장 공간을 절약한다.</li>
<li><strong>시각화:</strong> 인간이 직관적으로 이해할 수 있는 2차원이나 3차원으로 고차원 데이터를 축소하여, 데이터의 숨겨진 구조나 군집을 시각적으로 탐색할 수 있게 한다. </li>
</ul>
<h3 id="section-10-2">주요 접근법</h3>
<p>차원 축소는 크게 두 가지 접근법으로 나눌 수 있다. </p>
<ol>
<li><strong>특성 선택 (Feature Selection):</strong> 기존의 속성들 중에서 모델 성능에 가장 중요하다고 판단되는 속성들만 &#39;선택&#39;하고 나머지는 버리는 방식이다. 예를 들어, 속성 중요도 점수가 높은 상위 10개 속성만 사용하는 것이 여기에 해당한다. 원래 속성을 그대로 사용하므로 해석이 용이하다는 장점이 있다.</li>
<li><strong>특성 추출 (Feature Extraction):</strong> 기존의 여러 속성들을 조합하여 새로운 저차원의 속성을 &#39;추출&#39;하는 방식이다. 새로운 속성은 기존 속성들의 정보를 압축하여 담고 있다. 이 글에서 다룰 **주성분 분석(PCA)**이 대표적인 특성 추출 기법이다.</li>
</ol>
</section>
<section id="pca">
<h2 id="section-11">주성분 분석 (PCA): 데이터의 핵심을 꿰뚫는 선형 변환</h2>
<p>주성분 분석(Principal Component Analysis, PCA)은 가장 널리 알려지고 사용되는 특성 추출 기반의 차원 축소 기법이다. PCA의 목표는 <a href="https://www.ibm.com/kr-ko/think/topics/principal-component-analysis" target="_blank">원본 데이터가 가진 정보(분산)를 최대한 보존하면서, 서로 직교하는 새로운 축(주성분)을 찾아 고차원 데이터를 저차원으로 변환</a>하는 것이다.</p>
<h3 id="section-11-1">핵심 원리: 분산을 최대로 보존하는 축 찾기</h3>
<p>PCA의 원리를 비유를 통해 이해해보자. 하늘에 흩뿌려진 3차원 구름 데이터가 있다고 상상해보자. 이 구름의 모양을 2차원 평면에 가장 잘 표현하려면 어떻게 해야 할까? 즉, 어떤 각도에서 사진을 찍어야 구름의 형태가 가장 잘 드러날까? 아마도 구름이 가장 넓게 퍼져 있는 방향에서 사진을 찍어야 할 것이다. PCA는 바로 이 &#39;최적의 각도&#39;를 수학적으로 찾아내는 과정과 같다.</p>
<ul>
<li><strong>첫 번째 주성분 (Principal Component 1, PC1):</strong> 데이터가 가장 넓게 퍼져 있는 방향, 즉 **분산(Variance)이 가장 큰 방향**을 나타내는 축이다. PC1은 원본 데이터의 정보를 가장 많이 함축하고 있는 가장 중요한 축이다.</li>
<li><strong>두 번째 주성분 (Principal Component 2, PC2):</strong> PC1에 **직교(orthogonal)**하는 방향 중에서, 남아있는 데이터들의 분산이 가장 큰 방향을 나타내는 축이다.</li>
<li><strong>이후 주성분들:</strong> 이 과정을 반복하여 기존의 모든 주성분들과 직교하면서 분산을 최대로 하는 새로운 축들을 차례로 찾아낸다.</li>
</ul>
<p>이렇게 찾아낸 주성분 축들로 새로운 좌표계를 만든 뒤, 우리가 원하는 차원의 수만큼(예: 2차원으로 축소하고 싶다면 PC1과 PC2만) 선택하여 원본 데이터를 이 새로운 축에 투영(projection)시키면 차원 축소가 완료된다. <a href="https://m.blog.naver.com/angryking/222480031842" target="_blank">결과적으로, 원래의 수많은 변수들은 정보 손실을 최소화하며 몇 개의 주성분으로 압축된다.</a></p>
<h3 id="section-11-2">수학적 배경 (개념적)</h3>
<p>PCA의 수학적 구현은 선형대수학에 깊이 연관되어 있다. 간략한 개념은 다음과 같다. </p>
<ol>
<li>데이터의 각 속성들이 서로 얼마나 얽혀있는지를 나타내는 **공분산 행렬(Covariance Matrix)**을 계산한다.</li>
<li>이 공분산 행렬의 **고유값(Eigenvalue)**과 **고유벡터(Eigenvector)**를 찾는다.</li>
<li>여기서 고유벡터는 데이터의 분산이 큰 방향을 나타내는 &#39;축&#39;이 되고, 고유값은 해당 축 방향으로 데이터가 퍼진 정도, 즉 &#39;분산의 크기&#39;가 된다.</li>
<li>고유값이 큰 순서대로 고유벡터를 정렬한 것이 바로 주성분(PC1, PC2, ...)이 된다.</li>
</ol>
<h3 id="section-11-3">장점과 단점</h3>
<div class="key-points">
<h3 id="section-11-4">핵심 요약</h3>
<p><strong>장점:</strong></p>
<ul>
<li><strong>비지도 학습:</strong> 클래스 레이블이 없는 데이터에도 적용할 수 있다.</li>
<li><strong>계산 효율성:</strong> <a href="https://www.keboola.com/blog/pca-machine-learning" target="_blank">선형대수학에 기반하여 계산이 비교적 간단하고 빠르다.</a></li>
<li><strong>노이즈 제거:</strong> 분산이 작은 축(보통 노이즈에 해당)을 제거함으로써 데이터의 노이즈를 줄이는 효과가 있다.</li>
</ul>
<p><strong>단점:</strong></p>
<ul>
<li><strong>해석의 어려움:</strong> <a href="https://blog.dailydoseofds.com/p/the-advantages-and-disadvantages" target="_blank">변환된 주성분은 기존 속성들의 선형 결합으로 이루어져 있어</a>, 각 주성분이 어떤 의미를 갖는지 직관적으로 해석하기 어렵다.</li>
<li><strong>선형성 가정:</strong> PCA는 데이터의 구조가 선형적이라고 가정한다. 데이터가 복잡한 비선형 구조(예: 나선형, 원형)를 가질 경우, 그 구조를 제대로 파악하지 못할 수 있다.</li>
<li><strong>스케일 민감성:</strong> 변수들의 스케일(단위)에 영향을 많이 받으므로, PCA를 적용하기 전에는 반드시 각 변수를 표준화(Standardization)하는 전처리 과정이 필요하다.</li>
</ul>
</div>
</section>
<section id="beyond-pca">
<h2 id="section-12">PCA를 넘어서: 다양한 차원 축소 기법들</h2>
<p>PCA는 강력하고 널리 사용되는 차원 축소 기법이지만, 만능은 아니다. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6586259/" target="_blank">데이터의 특성과 분석 목적에 따라 더 적합한 다른 기법들을 고려</a>해야 한다. &#34;어떤 차원 축소 방법이 가장 좋은가?&#34;라는 질문에는 정답이 없으며, 문제에 따라 최적의 도구는 달라진다. 여기서는 PCA의 한계를 보완하거나 다른 관점에서 접근하는 주요 차원 축소 기법들을 소개한다.</p>
<h3 id="section-lda">선형 판별 분석 (LDA, Linear Discriminant Analysis)</h3>
<p>LDA는 PCA와 자주 비교되는 기법이지만, 근본적인 철학이 다르다. 가장 큰 차이점은 LDA가 **지도 학습(Supervised)** 기반의 차원 축소 기법이라는 점이다.</p>
<ul>
<li><strong>PCA와의 비교:</strong> PCA는 레이블 정보 없이 데이터 자체의 분산을 최대화하는 방향으로 축을 찾는다. 반면, LDA는 **클래스(레이블) 간의 분리는 최대화하고, 동일 클래스 내의 분산은 최소화**하는 방향으로 데이터를 투영할 축을 찾는다. </li>
<li><strong>활용:</strong> LDA의 목표는 &#39;분류&#39;를 가장 잘 할 수 있도록 차원을 축소하는 것이다. 따라서, 차원 축소의 주된 목적이 데이터 시각화나 탐색이 아니라, 이후 분류 모델의 성능을 높이는 것이라면 PCA보다 LDA가 더 효과적인 선택일 수 있다.</li>
</ul>
<h3 id="section-tsne">t-SNE (t-Distributed Stochastic Neighbor Embedding)</h3>
<p>t-SNE는 복잡한 고차원 데이터의 구조를 저차원(주로 2D 또는 3D)으로 시각화하는 데 매우 강력한 성능을 보이는 **비선형(Non-linear)** 차원 축소 기법이다.</p>
<ul>
<li><strong>특징:</strong> t-SNE의 핵심 아이디어는 <a href="https://rukshanpramoditha.medium.com/autoencoders-vs-t-sne-for-dimensionality-reduction-92dd1c6fe2d1" target="_blank">고차원 공간에서 가까웠던 데이터 포인트들이 저차원 공간에서도 가깝게 유지되도록</a> 하는 것이다. 즉, 데이터의 &#39;지역적 구조(Local Structure)&#39;를 보존하는 데 중점을 둔다.</li>
<li><strong>주요 용도:</strong> PCA로는 파악하기 어려운 복잡한 매니폴드(Manifold) 구조나 군집을 시각적으로 표현하는 데 탁월하다. <a href="https://www.datacamp.com/tutorial/introduction-t-sne" target="_blank">이미지, 음성, 생물학 데이터 등에서 숨겨진 패턴을 발견</a>하는 데 널리 사용된다.</li>
<li><strong>주의점:</strong> t-SNE는 몇 가지 해석상의 주의가 필요하다. 첫째, <a href="https://aicompetence.org/the-hidden-dangers-of-t-sne-avoid-pitfalls/" target="_blank">계산 복잡도가 높아(O(N²)) 대용량 데이터셋에는 적용하기 어렵다.</a> 둘째, 결과로 나타난 군집의 크기나 군집 간의 상대적인 거리가 실제 데이터의 특성을 그대로 반영하지 않을 수 있다. t-SNE는 군집화 알고리즘이 아니라 시각화 도구라는 점을 명심해야 한다.</li>
</ul>
<h3 id="section-autoencoders">오토인코더 (Autoencoders)</h3>
<p>오토인코더는 인공 신경망을 이용한 비선형 차원 축소 기법으로, 딥러닝 시대에 더욱 주목받고 있다.</p>
<ul>
<li><strong>구조:</strong> 오토인코더는 두 개의 신경망, 즉 **인코더(Encoder)**와 **디코더(Decoder)**로 구성된다. 
                        <ul>
<li><strong>인코더:</strong> 입력 데이터를 저차원의 잠재 공간(Latent Space)으로 압축하는 역할을 한다. 이 과정에서 데이터의 핵심 특징이 추출된다.</li>
<li><strong>디코더:</strong> 압축된 잠재 공간 표현으로부터 원본 데이터를 최대한 가깝게 복원하는 역할을 한다.</li>
</ul>
</li>
<li><strong>학습 원리:</strong> 모델은 &#39;입력&#39;과 &#39;복원된 출력&#39; 간의 차이(재구성 오류, Reconstruction Error)를 최소화하는 방향으로 학습된다. 이 과정에서 인코더는 데이터를 가장 효율적으로 압축하는 방법을 학습하게 된다. 학습이 완료되면, 우리는 디코더를 버리고 인코더 부분만을 사용하여 고차원 데이터를 저차원의 잠재 벡터로 변환하는 차원 축소기로 활용할 수 있다.</li>
<li><strong>활용:</strong> <a href="https://medium.com/@hassaanidrees7/autoencoders-vs-pca-dimensionality-reduction-for-complex-data-e07d4612b711" target="_blank">PCA가 포착하기 어려운 매우 복잡하고 비선형적인 데이터의 특징을 효과적으로 추출</a>할 수 있다. 이미지 압축, 노이즈 제거(Denoising Autoencoder), 이상 탐지, 생성 모델(Variational Autoencoder) 등 매우 다양한 분야에 응용된다.</li>
</ul>
</section>
<section id="conclusion">
<h2 id="section-13">결론: 알고리즘을 이해하고 현명하게 선택하기</h2>
<p>우리는 이 글을 통해 머신러닝의 핵심 알고리즘들을 깊이 있게 탐험했다. 스무고개처럼 직관적인 **결정 트리**에서 시작하여, &#39;다양성&#39;과 &#39;협력&#39;을 통해 성능을 극대화하는 **앙상블 기법**인 **랜덤 포레스트**와 **에이다부스트**의 작동 원리를 파헤쳤다. 나아가 데이터의 본질을 꿰뚫어 복잡성을 줄이는 **차원 축소**의 필요성을 이해하고, 그 대표 주자인 **PCA**부터 **LDA, t-SNE, 오토인코더**에 이르기까지 각 기법의 철학과 장단점을 비교 분석했다.</p>
<p>이 여정의 끝에서 우리가 도달하는 가장 중요한 결론은 &#34;공짜 점심은 없다(No Free Lunch Theorem)&#34;는 것이다. <a href="https://m.blog.naver.com/nilsine11202/221577750855" target="_blank">모든 상황과 모든 데이터에 완벽하게 들어맞는 최강의 알고리즘은 존재하지 않는다.</a> 어떤 모델이 최적인지는 전적으로 우리가 해결하려는 문제와 우리가 가진 데이터의 특성에 달려 있다.</p>
<ul>
<li><strong>해석 가능성</strong>이 중요하다면, 단일 결정 트리나 선형 모델이 좋은 출발점이 될 수 있다.</li>
<li><strong>예측 성능</strong>을 극한으로 끌어올려야 한다면, 랜덤 포레스트나 그래디언트 부스팅과 같은 앙상블 모델이 강력한 무기가 될 것이다.</li>
<li>데이터가 **선형 구조**를 가진다면 PCA가 효율적인 차원 축소 방법이겠지만, 복잡한 **비선형 구조**를 가졌다면 t-SNE나 오토인코더를 고려해야 한다.</li>
<li>**분류 성능**을 높이는 것이 차원 축소의 목적이라면, 비지도 방식인 PCA보다 지도 방식인 LDA가 더 나은 선택일 수 있다.</li>
</ul>
<p>결국 성공적인 머신러닝 프로젝트는 단순히 최신 라이브러리를 사용하는 능력에 있는 것이 아니라, 데이터의 특성을 이해하고, 문제의 목표를 명확히 정의하며, 각 알고리즘의 강점과 약점을 바탕으로 가장 적합한 도구를 현명하게 선택하고 조합하는 능력에 있다. 이 글에서 다룬 기본 알고리즘들에 대한 깊은 이해는 앞으로 마주할 더 복잡하고 새로운 모델들을 학습하고 응용하는 데 있어 흔들리지 않는 튼튼한 기반이 되어줄 것이다.</p>
</section>
<div class="reference-container">
			<div class="reference-section">
				<h3 class="reference-title">참고 자료</h3>
				<div class="reference-list">
					
					<div class="reference-item">
						<span class="reference-number">[1]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">6. 주성분 분석(Principal Component Analysis, PCA)과 선형 ...</span>
									
							</div>
							<a href="https://m.blog.naver.com/tjdqja0508/222309214934" class="reference-link" target="_blank">
								https://m.blog.naver.com/tjdqja0508/222309214934
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[2]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">[인공지능 기초] 결정 트리(Decision Tree)</span>
									
							</div>
							<a href="https://blog.naver.com/jgyy4775/222629452259" class="reference-link" target="_blank">
								https://blog.naver.com/jgyy4775/222629452259
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[3]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">[Machine Learning] 지니 불순도(Gini Impurity) 이해하기</span>
									
							</div>
							<a href="https://datasciencebeehive.tistory.com/84" class="reference-link" target="_blank">
								https://datasciencebeehive.tistory.com/84
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[4]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">결정 트리 (Decision Tree) - velog</span>
									
							</div>
							<a href="https://velog.io/@lsy0476/%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-Decision-Tree" class="reference-link" target="_blank">
								https://velog.io/@lsy0476/%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-Decision-Tree
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[5]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">부스팅 앙상블 (Boosting Ensemble) 1: AdaBoost</span>
									
							</div>
							<a href="https://tyami.github.io/machine%20learning/ensemble-3-boosting-AdaBoost/" class="reference-link" target="_blank">
								https://tyami.github.io/machine%20learning/ensemble-3-boosting-AdaBoost/
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[6]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">Differences between Random Forest and AdaBoost - GeeksforGeeks</span>
									
							</div>
							<a href="https://www.geeksforgeeks.org/machine-learning/differences-between-random-forest-and-adaboost/" class="reference-link" target="_blank">
								https://www.geeksforgeeks.org/machine-learning/differences-between-random-forest-and-adaboost/
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[7]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">[머신러닝]앙상블(Ensemble) - Bagging, RandomForest, Boosting ...</span>
									
							</div>
							<a href="https://topo314.tistory.com/81" class="reference-link" target="_blank">
								https://topo314.tistory.com/81
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[8]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">주성분 분석(PCA) 개념과 원리 쉽게 이해하기 - 네이버 블로그</span>
									
							</div>
							<a href="https://m.blog.naver.com/angryking/222480031842" class="reference-link" target="_blank">
								https://m.blog.naver.com/angryking/222480031842
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[9]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">PCA (Principal Component Analysis) : 주성분 분석 이란?</span>
									
							</div>
							<a href="https://ddongwon.tistory.com/114" class="reference-link" target="_blank">
								https://ddongwon.tistory.com/114
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[10]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">Autoencoders vs t-SNE for Dimensionality Reduction</span>
									
							</div>
							<a href="https://rukshanpramoditha.medium.com/autoencoders-vs-t-sne-for-dimensionality-reduction-92dd1c6fe2d1" class="reference-link" target="_blank">
								https://rukshanpramoditha.medium.com/autoencoders-vs-t-sne-for-dimensionality-reduction-92dd1c6fe2d1
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[11]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">Comparing T-SNE And UMAP: When To Use One Over The Other - AI</span>
									
							</div>
							<a href="https://aicompetence.org/comparing-t-sne-and-umap/" class="reference-link" target="_blank">
								https://aicompetence.org/comparing-t-sne-and-umap/
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[12]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">Autoencoders vs. PCA: Dimensionality Reduction for Complex Data</span>
									
							</div>
							<a href="https://medium.com/@hassaanidrees7/autoencoders-vs-pca-dimensionality-reduction-for-complex-data-e07d4612b711" class="reference-link" target="_blank">
								https://medium.com/@hassaanidrees7/autoencoders-vs-pca-dimensionality-reduction-for-complex-data-e07d4612b711
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[13]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">차원 축소 - PCA, 주성분분석 (1) - EXCELSIOR - 티스토리</span>
									
							</div>
							<a href="https://excelsior-cjh.tistory.com/167" class="reference-link" target="_blank">
								https://excelsior-cjh.tistory.com/167
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[14]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">머신 러닝 : Random Forest 특징, 개념, 장점, 단점 - 쵸코쿠키의 연습장</span>
									
							</div>
							<a href="https://jjeongil.tistory.com/908" class="reference-link" target="_blank">
								https://jjeongil.tistory.com/908
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[15]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">13. 랜덤 포레스트 (Random Forest) / 에이다부스트 (AdaBoost)</span>
									
							</div>
							<a href="https://goatlab.tistory.com/entry/13-%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-Random-Forest-%EC%97%90%EC%9D%B4%EB%8B%A4%EB%B6%80%EC%8A%A4%ED%8A%B8-AdaBoost" class="reference-link" target="_blank">
								https://goatlab.tistory.com/entry/13-%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-Random-Forest-%EC%97%90%EC%9D%B4%EB%8B%A4%EB%B6%80%EC%8A%A4%ED%8A%B8-AdaBoost
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[16]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">Boosting 알고리즘 - Adaboost 동작 원리</span>
									
							</div>
							<a href="https://bommbom.tistory.com/entry/Boosting-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-Adaboost-%EB%8F%99%EC%9E%91-%EC%9B%90%EB%A6%AC" class="reference-link" target="_blank">
								https://bommbom.tistory.com/entry/Boosting-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-Adaboost-%EB%8F%99%EC%9E%91-%EC%9B%90%EB%A6%AC
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[17]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">Principal Component Analysis(PCA) - GeeksforGeeks</span>
									
							</div>
							<a href="https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-pca/" class="reference-link" target="_blank">
								https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-pca/
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[18]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">단어 임베딩 차원 축소: 효과적인 데이터 시각화 전략</span>
									
							</div>
							<a href="https://awetimelegend.tistory.com/entry/%EB%8B%A8%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-%EC%B0%A8%EC%9B%90-%EC%B6%95%EC%86%8C-%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%8B%9C%EA%B0%81%ED%99%94-%EC%A0%84%EB%9E%B5" class="reference-link" target="_blank">
								https://awetimelegend.tistory.com/entry/%EB%8B%A8%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-%EC%B0%A8%EC%9B%90-%EC%B6%95%EC%86%8C-%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%8B%9C%EA%B0%81%ED%99%94-%EC%A0%84%EB%9E%B5
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[19]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">Dimensionality reduction - Wikipedia</span>
									
							</div>
							<a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" class="reference-link" target="_blank">
								https://en.wikipedia.org/wiki/Dimensionality_reduction
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[20]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">PCA(주성분분석) vs LDA(선형판별분석) 비교 - Woomii&#39;s story</span>
									
							</div>
							<a href="https://woomii.tistory.com/60" class="reference-link" target="_blank">
								https://woomii.tistory.com/60
							</a>
						</div>
					</div>
					
					<div class="reference-item">
						<span class="reference-number">[21]</span>
						<div class="reference-content">
							<div class="reference-text-container">
								<span class="reference-text">Autoencoder 를 이용한 차원 축소 (latent representation)</span>
									
							</div>
							<a href="https://dodonam.tistory.com/301" class="reference-link" target="_blank">
								https://dodonam.tistory.com/301
							</a>
						</div>
					</div>
					
				</div>
			</div>
		</div></div>
<script>
        document.addEventListener('DOMContentLoaded', function () {
            // Feature Importance Chart
            const ctx = document.getElementById('featureImportanceChart').getContext('2d');
            const featureImportanceChart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['feature 1', 'feature 0', 'feature 2', 'feature 7', 'feature 8', 'feature 6', 'feature 4', 'feature 3', 'feature 9', 'feature 5'],
                    datasets: [{
                        label: '속성 중요도 (Mean Decrease in Impurity)',
                        data: [0.32, 0.21, 0.19, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.03],
                        backgroundColor: 'rgba(54, 162, 235, 0.6)',
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 1
                    }]
                },
                options: {
                    indexAxis: 'y',
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        title: {
                            display: true,
                            text: '결정 트리 속성 중요도 예시',
                            font: {
                                size: 18
                            }
                        },
                        legend: {
                            display: false
                        }
                    },
                    scales: {
                        x: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: '중요도 값'
                            }
                        }
                    }
                }
            });
        });
    </script>

</body></html>